{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbd180ea-edbe-4ddd-a7f9-433d0b8adb49",
   "metadata": {},
   "source": [
    "<div align='center'><font size=\"6\">Assesment for Prompt Engineering Role</font></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d83ce33-c7cc-4821-8200-94240cf6388b",
   "metadata": {},
   "source": [
    "## Assignment-1(Natural Language Processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9748d460-9da0-4dd3-99a0-28e47da1d71a",
   "metadata": {},
   "source": [
    "Aim:\n",
    "    To Implement functions to preprocess and tokenize text data with the help of NLTK functionalities.\n",
    "\n",
    "Procedure:\n",
    "    \n",
    "    *Data Cleaning:Making all text either to  lower case or uppercase format and Removin noise(punctuations,links,quoted texts,etc.,)\n",
    "    \n",
    "    *Tokenisation:Tokenization is a process that splits an input sequence into so-called tokens where the tokens can be a word, sentence, paragraph etc. Base upon the type of tokens we want, tokenization can be of various types.\n",
    "    \n",
    "    *Stop-Words Removal:Stop-words are the words which occur very frequently but have no possible value. For example,(a, an, the, are, etc).Removing stopwords is one of the important step of preprocessing.\n",
    "\n",
    "Result:\n",
    "    In the below cells,we have written the user-defined functions successfully, to implement all the preprocessing and tokenisation functionalities.e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ad18c0-1079-4d94-a8fa-acf04de747d5",
   "metadata": {},
   "source": [
    "### Importing Necessary Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a51c861-df8e-4dc7-9a0c-9125cd6d97d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb27845a-7f2f-45c0-a77a-712a439fd4ab",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e259472-2042-49cc-9460-78ea6b43811d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "input_data=input()#Enter the text or list of texts to be preprocessed and tokenized here.\n",
    "data = input_data.apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809b2b8c-a364-4241-b61e-f147e91cde05",
   "metadata": {},
   "source": [
    "### Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed57673-bf23-48d0-9cb0-1d47b2299ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')#One can use any required tokeniser.I have used this tokeniser for instance.\n",
    "tokenized_text = data.apply(lambda x: tokenizer.tokenize(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea617a57-cee0-4c8a-80b8-7faded86b3c6",
   "metadata": {},
   "source": [
    "### Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b14c421-97b2-41b5-acb8-c94984ed037b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    \"\"\"\n",
    "    Removing stopwords belonging to english language\n",
    "    \"\"\"\n",
    "    words = [w for w in text if w not in stopwords.words('english')]\n",
    "    return words\n",
    "\n",
    "result= tokenized_text.apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78994649-5d32-4028-a0bc-06eb933be758",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e242e15-01b7-47ea-b71a-c589ba9c23c3",
   "metadata": {},
   "source": [
    "## Assignment-2(Text Generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63c8f3f-5790-4551-9ae4-1054ad34449b",
   "metadata": {},
   "source": [
    "Aim:\n",
    "    To Create a basic text generation model using a pre-trained transformer (e.g., GPT-3).\n",
    "\n",
    "Procedure:\n",
    "\n",
    "    *Importing Hugging Face Transformers library as per requisites and other necessary packages.\n",
    "\n",
    "    *Choosing the apt model and tokenizer.Then initialising them for the process.\n",
    "\n",
    "    *Text Generation Function:The input prompt will be encoded here.Then this function generates text as per the request.Decoding of the output text(generated text) also takes place here only.\n",
    "\n",
    "Result:\n",
    "       In the below cells,the text generation model has been built successfully and it generates coherent text based on a given prompt.\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743a4e6a-f6e4-427c-9794-f2d75bfb3b8b",
   "metadata": {},
   "source": [
    "### Importing Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38c3bfe-bbe9-48da-922d-e6a3f3babf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b1f14c-febc-4ef4-a973-4d21c8d6d3aa",
   "metadata": {},
   "source": [
    "### Loading Pre-trained model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5c93af-48a0-4ba7-a9cd-cf58f4c9799c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt3\" #One can replace 'gpt3' with whatever model name as their wish and the access they have.I have used gpt-3 here as per assignment requirement.\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name) \n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b7f3cc-edbc-41a2-ae17-512c78705a73",
   "metadata": {},
   "source": [
    "### Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591bb704-dd55-48fc-a2b9-b0cf0fa49784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_length=100): #Max length is customisable as per requirement. \n",
    "    \n",
    "    input = tokenizer.encode(prompt, return_tensors='pt') # Encoding the prompt\n",
    "    \n",
    "    output = model.generate(input, max_length=max_length, num_return_sequences=1)\n",
    "    '''\n",
    "    Output will be encoded.So we have to decode it in human understandable form,that is performed below.\n",
    "    '''\n",
    "    \n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True) #The generated text will be the decoded output with respect to user-prompt.\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5757d6c-c03d-4046-acad-da10c55f3baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = input() #Enter the input prompt here manually or enter it at the time of pop-up during execution.\n",
    "generated_text = generate_text(prompt, max_length=100) \n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4944a4-2a68-48d3-8954-f39e3c19810b",
   "metadata": {},
   "source": [
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab8724e-2886-4301-a721-eba26df4b667",
   "metadata": {},
   "source": [
    "## Assignment-3(Prompt Engineering)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc12603-6f56-4d14-8aa4-170a972e665a",
   "metadata": {},
   "source": [
    "AIM:\n",
    "    To design and evaluate prompts to improve the performance of a AI model on tasks like summarization and question answering.\n",
    "\n",
    "Procedure:\n",
    "\n",
    "          *Here,I have generated an API key for secure connection, in respect to sending request to openAI and to get respond from them, in my interface.Then I have exported the API key as an environment variable in my IDE.This step should be performed by each individual or if you are working under an organisation they will provide the API key, as it would been already generated by the devOPS team.\n",
    "\n",
    "          *After this we have to import the necessary packages.Then we should create different templates of prompts for summarisation and question answering.I have used two different designs each for summarisation and question answering.\n",
    "\n",
    "          *Model Selection:I have used OpenAI(Completion) model for this assignment to generate outputs based on the user-input prompts by creating a connection to the respective engine.I have written a function to call OpenAI API to generate answers based on prompt.\n",
    "\n",
    "          *Evaluation:I have used BLEU,ROUGE evaluation metrics to evaluate summarisation responses and accuracy,F1 score for question answering responses.The evaluation takes place by comparing the generated output  and the correct output.In this case,since I have no data sets containing the (text_for_summary and reference_summary considering summarisation) and (context_for_qa,reference_answer and question_for_qa for question answering) I have did it by giving manually to do the model evaluation.\n",
    "\n",
    "          *Note:While seeing the code you can get clear idea on what I have specified in the above point.Generation of output is performed from openAI.But to evaluate it we should have pre-defined data sets comprises of testing features that I have mentioned above.The model(OpenAI) has already been trained well.To evaluate it with testing data sets we are in lack of data set.So I have given testing dataset manually.Here we can give pre-defined data sets also.For time-being I have given manually.\n",
    "\n",
    "Result:\n",
    "       The objective of this assignment has been achieved.With increased number of testing data we can do the evaluation in great manner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5483ad-2b6b-4029-bd83-a4c05b1aba82",
   "metadata": {},
   "source": [
    "## Exporting API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685a4571-8a09-4fac-85d3-c7aeed71c0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "setx OPENAI_API_KEY \"e7062c5b-d95d-4fa5-af31-52cb6e662816\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e4ef81-1ae1-4673-9d5b-1a6890372c1c",
   "metadata": {},
   "source": [
    "### Importing Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e749a060-1ee7-4cc4-b86d-3f97b1cc35a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai \n",
    "from sklearn.metrics import f1_score \n",
    "from nltk.translate.bleu_score \n",
    "import sentence_bleu\n",
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19dedd5-d1e5-487d-94b6-7d8f90f50816",
   "metadata": {},
   "source": [
    "### Setting OpenAI API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455d0e0c-10ec-4523-b0ed-25b95fe3f41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = 'e7062c5b-d95d-4fa5-af31-52cb6e662816'#Give your API Key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d99ee3-faf0-4686-8dea-7bfc1d510977",
   "metadata": {},
   "source": [
    "### Defining Different Prompt Designs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39634e84-979d-49d8-9a11-e4ea43baf39e",
   "metadata": {},
   "outputs": [],
   "source": [
    " prompts = { \"summarization_1\": \"Summarize the following text: {}\", \n",
    "            \"summarization_2\": \"Provide a concise summary of this text: {}\", \n",
    "            \"question_answering_1\": \"What is the answer to the following question based on this context? Context: {} Question: {}\", \n",
    "            \"question_answering_2\": \"Based on this information: {} What is your answer to this question: {}\" \n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6354eb6a-13e3-4c09-97a3-250c54ba1db7",
   "metadata": {},
   "source": [
    "### Function to call OpenAI Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb1ded0-bd5d-474c-a5d5-ed075ffc0edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt, text): #Function to generate response\n",
    "    response = openai.Completion.create( engine=\"text-davinci-003\", \n",
    "                                        prompt=prompt.format(text), \n",
    "                                        max_tokens=150, n=1, stop=None, temperature=0.7 ) \n",
    "    return response.choices[0].text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4db44d-1228-4431-9249-a6d48e94cf71",
   "metadata": {},
   "source": [
    "### Function to evaluate summarisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e783b8-d036-4b50-9d18-54ca0dbe6a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_summarization(reference_summary, generated_summary): \n",
    "    bleu_score = sentence_bleu([reference_summary.split()], generated_summary.split()) \n",
    "    rouge = Rouge() \n",
    "    rouge_score = rouge.get_scores(generated_summary, reference_summary)[0] \n",
    "    return bleu_score, rouge_score['rouge-l']['f']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0366c4e2-b567-4fab-aa1e-d373879b4799",
   "metadata": {},
   "source": [
    "### Function to evaluate Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113caab6-eaae-42de-bb98-6851b64985ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_qa(reference_answer, generated_answer): \n",
    "    reference = [reference_answer] \n",
    "    prediction = [generated_answer] \n",
    "    f1 = f1_score(reference, prediction, average='weighted')\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a623f4-df47-41c3-9dac-2abc8f3b5df2",
   "metadata": {},
   "source": [
    "### Example texts and corresponding references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52eddbb-e3fd-46e9-89d7-24acdfac809a",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Here I have manually given one testing data each for summarisation and QA Evaluation.One can give the list of testing data inorder to evaluate.\n",
    "If we are using list of testing data slight changes in the code should be performed corresponding to List, in Evaluation segments alone.\n",
    "\n",
    "'''\n",
    "\n",
    "text_for_summary = \"ChatGPT is a conversational agent developed by OpenAI that uses artificial intelligence to engage in dialogue with users.\" \n",
    "reference_summary = \"ChatGPT is developed by OpenAI as a conversational AI.\" \n",
    "\n",
    "\n",
    "context_for_qa = \"The capital of France is Paris.\" \n",
    "reference_answer = \"Paris\" \n",
    "question_for_qa = \"What is the capital of France?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9eb50d8-5795-4ed1-afa9-df52bb4c4586",
   "metadata": {},
   "source": [
    "### Evaluating prompt designs(Summarisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c944f5a3-c912-482a-8d75-e2808fab0ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt_key, prompt_template in prompts.items(): #Iterating the dictionary elements one by one \n",
    "    if 'summarization' in prompt_key: #To choose summarisation design prompts for evaluation\n",
    "        generated_summary = generate_response(prompt_template, text_for_summary)#Calling the function to generate respose from openAI \n",
    "        bleu, rouge = evaluate_summarization(reference_summary, generated_summary)#Evaluation\n",
    "        print(f\"{prompt_key} - BLEU: {bleu}, ROUGE: {rouge}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330669ea-6cde-4951-964b-c9642572a4dd",
   "metadata": {},
   "source": [
    "### Evaluating prompt designs(Question Answering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbf10ea-f94c-4c10-a3ba-5fa748679755",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt_key, prompt_template in prompts.items():\n",
    "    if 'question_answering' in prompt_key:#To choose QA design prompts for generate response and evaluation\n",
    "        generated_answer = generate_response(prompt_template, context_for_qa + \" \" + question_for_qa)#Caliing the function to generate response \n",
    "        f1 = evaluate_qa(reference_answer, generated_answer) #Evaluation\n",
    "        print(f\"{prompt_key} - F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb77313-bd22-4353-8896-ac4e4a586e28",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ba46e8-7eb7-489a-a266-b1623bba2a20",
   "metadata": {},
   "source": [
    "## Assignment 4-(Data Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a23399-be14-4dd7-826c-cd1c67998328",
   "metadata": {},
   "source": [
    "Aim:\n",
    "    To Analyze a dataset(Wine Quality from UCI-ML Repository) and generate insights using a combination of descriptive statistics and visualizations\n",
    "\n",
    "Procedure:\n",
    "\n",
    "        *First we have to import the necessary packages.Then loading data comes to the place.One can download the data from repository and keep the dataset in the local repository.Then we can load it directly or we can load the data by giving the URL of the dataset directly.Loading can be done with the help of Pandas in any of one methodolies for this task\n",
    "\n",
    "        *Then we can start with the EDA(Exploratory Data Analysis) to know about data.Descriptive Statistics will also be done in this segment.With these analysis we can get the Quality count that is the count of number of occurences of each wine quality rating in this case.\n",
    "\n",
    "        *Visualisation:To visualise the Analysis and insights from the Data I have used three plots,they are pairplot,box plot and heatmap each have their unique functionalities.Usage of pairplot-To display relationship between selected features and wine quality.Usage of Boxplot-To examine the distribution of alcohol content accross different wine quality ranges.Finally,Usage of Heatmap-To display the correleation matrix for all features in the data set.\n",
    "\n",
    "\n",
    "Result:\n",
    "       The analysation and visualisation of the wine dataset has been done successfully.The data-driven insights have also been derived from the whole process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3e717d-97d2-41d5-b6ea-c7f098f404d2",
   "metadata": {},
   "source": [
    "### Importing Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e40932-1149-466e-8470-cafa355a1e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec04d553-a901-4f63-8818-4423466d3c91",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ee5bd3-6cfc-4709-8994-883c1f795533",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\" \n",
    "wine_data = pd.read_csv(url, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64316577-4be2-4c4c-be64-fd51ff136a42",
   "metadata": {},
   "source": [
    "### EDA and descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68c4e35-f505-49d6-be70-9ec488d4aea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wine_data.head()) #To get idea about the data distribution, printing the first 5 rows \n",
    "\n",
    "summary_statistics = wine_data.describe() #Summary Statistics\n",
    "print(\"\\nSummary Statistics:\\n\", summary_statistics)  \n",
    "\n",
    "quality_count = wine_data['quality'].value_counts() # Count of wine quality ratings\n",
    "print(\"\\nQuality Count:\\n\", quality_count) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a94d66-ecb7-4b6f-93f1-760cde1d0924",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddb9de1-0618-4bed-bb65-4cb9391e0769",
   "metadata": {},
   "source": [
    "### Pairplot(For Visualising Relationship)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6b9bd3-5359-4c62-8b23-b8663a70d461",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(wine_data, hue='quality', \n",
    "             vars=['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'alcohol']) #Selecting only few needed features\n",
    "plt.title('Pairplot of Wine Quality Dataset (Selected Features)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e910776e-44cf-4b08-a822-3ef0f0c2a781",
   "metadata": {},
   "source": [
    "### Boxplot(For Visaulising alcohol content by quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9603e35f-46ca-496d-b8cb-f130d9bfae99",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6)) #Size of the plot\n",
    "sns.boxplot(x='quality', y='alcohol', data=wine_data)\n",
    "plt.title('Boxplot of Alcohol Content by Wine Quality') \n",
    "plt.xlabel('Wine Quality')\n",
    "plt.ylabel('Alcohol Content')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77b7dab-096f-48e3-b6f4-a620fb388c4d",
   "metadata": {},
   "source": [
    "### Heatmap(Correlation Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8145df-1416-435e-9957-02a84d365837",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8)) \n",
    "correlation_matrix = wine_data.corr() \n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5) \n",
    "plt.title('Correlation Heatmap of Wine Quality Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0fa93e-77ab-419c-8be3-f8e086f39c8a",
   "metadata": {},
   "source": [
    "## Analysis Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958bfe83-557e-4056-ac0c-aa519474fe8a",
   "metadata": {},
   "source": [
    "1. Dataset Overview:\n",
    "- The dataset consists of 1,599 samples with 12 features(including the target variable,\"quality\"). \n",
    "- The features include physicochemical properties such as acidity, sugars, pH, and alcohol content. \n",
    "- The target variable \"quality\" ranges from 0 to 10, representing different quality ratings of the wine.\n",
    "\n",
    "2. Summary Statistics:\n",
    "- **Fixed Acidity**: Ranges from approximately 4.6 to 15.9 g/dm³, with a mean of around 7.22.\n",
    "- **Volatile Acidity**: Ranges from 0.12 to 1.58 g/dm³, suggesting that the wine's taste could be significantly affected by volatile acidity.\n",
    "- **Citric Acid**: Ranges from 0 to 1.0, with many wines having low citric acid levels.\n",
    "- **Residual Sugar**: Shows a wide range, with values from 0.9 to 15.5 g/dm³, indicating variability in sweetness levels.\n",
    "- **Alcohol Content**: Ranges between 8.0% and 14.9% with a mean of approx 10.5%, suggesting a general tendency towards moderate alcohol content.\n",
    "\n",
    "3. Quality Count Distribution:\n",
    "- Quality ratings 6 and 5 are the most frequent, each accounting for approximately 30% of the dataset.\n",
    "- Lower ratings (1-4) and higher ratings (7-10) are less common, indicating that most wines are perceived as average to good quality.\n",
    "\n",
    "4. Key Visual Insights: \n",
    "- **Pairplot Analysis**: The pairplot shows clear separation between different quality ratings for certain features like alcohol, volatile acidity, and citric acid, indicating these features are influential in determining wine quality.\n",
    "\n",
    "- **Boxplot Analysis of Alcohol Content**:\n",
    "    - Higher quality wines (ratings 7-8) tend to have higher alcohol content.\n",
    "    - Lower quality wines tend to show increased volatility in alcohol content, hinting at a possible degradation in quality.\n",
    "\n",
    "- **Correlation Heatmap**:\n",
    "\n",
    "      - **Positive Correlation**: Alcohol content shows a positive correlation with wine quality (correlation coefficient ≈ 0.48). Higher alcohol content is generally associated with better wine quality.\n",
    "\n",
    "      - **Negative Correlation**: Volatile acidity has a negative correlation with quality (correlation coefficient ≈ -0.32), suggesting higher volatile acidity leads to lower quality ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2927d1ac-0896-4850-8bb0-d274eddeddf9",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac080a9d-0d13-4d46-a8e5-9eb0c8b8d4e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
